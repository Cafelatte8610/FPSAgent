Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,2.8045137,2.0055855855855858,-0.5277199,-0.9343228808176987,-0.9343228808176987,0.23961411,0.02548871,0.00028461945,0.19487314,0.0047441702,1.0
100000,2.4762394,9.666880616174582,-0.8780007,-0.9189313333389951,-0.9189313333389951,0.05378256,0.022385718,0.00025692937,0.18564315,0.004283591,1.0
150000,2.1976202,70.7609841827768,-0.6632081,-0.7957674028211633,-0.7957674028211633,0.02364073,0.021967199,0.00022608643,0.17536211,0.0037705698,1.0
200000,2.0769644,403.00892857142856,-0.3999802,-0.5117017909485314,-0.5117017909485314,0.0069615818,0.025502166,0.0001952372,0.16507904,0.0032574437,1.0
250000,2.083788,593.919540229885,-0.2496069,-0.4428069168466261,-0.4428069168466261,0.004794412,0.024920035,0.00016443654,0.15481217,0.0027451268,1.0
300000,2.1123223,518.7010309278351,-0.121418186,-0.1833278563680108,-0.1833278563680108,0.0053046485,0.024022464,0.00013364504,0.14454831,0.0022329611,1.0
350000,2.1129825,653.4871794871794,-0.06791416,-0.30573509140061095,-0.30573509140061095,0.0029972554,0.022788195,0.00010284979,0.13428324,0.0017207334,1.0
400000,2.0996406,707.1486486486486,-0.021132065,-0.4742906945943832,-0.4742906945943832,0.0019616787,0.022062303,7.5137636e-05,0.12504585,0.0012597878,1.0
450000,2.02723,511.2696629213483,0.028779725,-0.3862629377105263,-0.3862629377105263,0.007757033,0.02415946,4.7397683e-05,0.11579921,0.0007983801,1.0
500000,1.955335,784.6031746031746,-0.019281931,-0.5625365421412483,-0.5625365421412483,0.0020008357,0.026097128,1.6575816e-05,0.10552523,0.00028570945,1.0
