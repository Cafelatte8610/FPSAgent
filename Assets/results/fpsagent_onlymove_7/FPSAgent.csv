Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
1000,2.1939745,499.5,0.011920012,1000.0,1000.0,1.0
2000,2.194959,999.0,-0.1818167,-180.93206548690796,-180.93206548690796,1.0
3000,2.1949909,999.0,-1.0888417,-137.92497062683105,-137.92497062683105,1.0
4000,2.1749513,999.0,0.422082,-131.59835308790207,-131.59835308790207,1.0
5000,2.1826842,999.0,-0.08951711,-183.12613606452942,-183.12613606452942,1.0
6000,2.1787372,999.0,-0.12136449,-218.7532033920288,-218.7532033920288,1.0
7000,2.1299438,999.0,-2.119416,-167.3599557876587,-167.3599557876587,1.0
8000,2.1923203,999.0,-0.8908739,-132.3572895526886,-132.3572895526886,1.0
9000,2.19404,999.0,-1.3032417,-225.66447019577026,-225.66447019577026,1.0
10000,2.1685102,999.0,-0.8499961,-206.1551284790039,-206.1551284790039,1.0
11000,2.1539423,999.0,-1.2104386,-181.8706785440445,-181.8706785440445,1.0
12000,2.1845205,999.0,-1.6675684,-153.00510156154633,-153.00510156154633,1.0
