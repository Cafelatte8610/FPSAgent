Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,2.6271625,1.8469289007798713,-0.02172636,-0.9755386199037247,-0.9755386199037247,31.127403,0.025249392,0.0002846256,0.19487518,0.0047442727,1.0
100000,2.56186,4.2700784064420425,-0.94538563,-0.9741417987232882,-0.9741417987232882,0.025601596,0.02600286,0.0002569253,0.1856418,0.004283524,1.0
150000,2.1545756,27.61997563946407,-0.7641892,-0.9680722389741826,-0.9680722389741826,0.024299541,0.023733323,0.00022611077,0.17537025,0.0037709747,1.0
200000,1.8410813,155.86666666666667,-0.34232345,-0.9918117335154897,-0.9918117335154897,0.020954188,0.025702069,0.00019529743,0.16509911,0.0032584462,1.0
250000,1.9719166,402.71774193548384,-0.20083456,-0.997889539286975,-0.997889539286975,0.00396541,0.024223093,0.00016448574,0.15482855,0.002745945,1.0
300000,2.0854936,486.0851063829787,-0.1626629,-0.9976851401176858,-0.9976851401176858,0.0011404826,0.021847578,0.00013365378,0.14455126,0.0022331069,1.0
350000,2.0220156,478.2818181818182,-0.16642049,-0.9979754871265455,-0.9979754871265455,0.0016004915,0.02393448,0.00010278608,0.13426201,0.0017196737,1.0
400000,2.0213606,488.47115384615387,-0.16456412,-0.9979144571205745,-0.9979144571205745,0.0016485574,0.023969524,7.500083e-05,0.12500024,0.0012575125,1.0
450000,2.0386956,487.6082474226804,-0.15017776,-0.9979917011999836,-0.9979917011999836,0.0019615367,0.024706868,4.7198253e-05,0.115732715,0.00079506275,1.0
500000,2.0224605,493.54455445544556,-0.14938465,-0.9979147403409668,-0.9979147403409668,0.0015448725,0.023804883,1.634818e-05,0.10544934,0.00028192313,1.0
