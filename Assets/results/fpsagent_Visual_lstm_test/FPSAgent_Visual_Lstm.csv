Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,2.8002942,2.057823960880196,-0.49301738,-0.9319552707046812,-0.9319552707046812,0.22788763,0.024011113,0.000284612,0.19487064,0.0047440454,1.0
100000,2.556836,6.659362549800797,-0.889189,-0.9327105979364384,-0.9327105979364384,0.058986418,0.02182798,0.000256938,0.18564601,0.004283736,1.0
150000,2.2790577,31.574197860962567,-0.80573,-0.8754371384079045,-0.8754371384079045,0.029826257,0.023295952,0.00022615647,0.17538545,0.0037717358,1.0
200000,2.1676605,253.67741935483872,-0.56074244,-0.7502294768794224,-0.7502294768794224,0.007971714,0.024414763,0.00019532142,0.1651071,0.0032588448,1.0
250000,2.123275,720.2647058823529,-0.33624622,-0.7415000264916349,-0.7415000264916349,0.0026479315,0.022333715,0.00016454332,0.15484777,0.0027469029,1.0
300000,2.1227164,693.1285714285714,-0.21488908,-0.5243714580578464,-0.5243714580578464,0.0033812418,0.02263942,0.0001337216,0.14457385,0.0022342345,1.0
350000,2.1295705,818.2741935483871,-0.15688889,-0.6865387456792016,-0.6865387456792016,0.001993698,0.022574747,0.000102874634,0.13429151,0.0017211467,1.0
400000,2.152325,706.92,-0.12409811,-0.5313546942174434,-0.5313546942174434,0.0021941918,0.023364456,7.5114236e-05,0.12503804,0.0012593984,1.0
450000,2.1292288,695.7941176470588,-0.09037618,-0.4173853230826995,-0.4173853230826995,0.0024892632,0.025724068,4.7317288e-05,0.11577239,0.00079704274,1.0
500000,2.1102486,758.776119402985,-0.081505865,-0.524758241292256,-0.524758241292256,0.0026572337,0.021759829,1.6453898e-05,0.105484605,0.0002836815,1.0
