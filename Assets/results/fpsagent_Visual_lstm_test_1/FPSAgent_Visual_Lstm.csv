Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,2.8106027,1.968373583338278,-0.5372398,-0.9316652624674162,-0.9316652624674162,0.2318987,0.027743671,0.00028460746,0.19486918,0.00474397,1.0
100000,2.5563304,6.442476559011758,-0.9033971,-0.9254140419174103,-0.9254140419174103,0.05284682,0.025680294,0.00025691866,0.18563956,0.004283414,1.0
150000,2.3333867,20.281671745755332,-0.79761934,-0.8717795365822056,-0.8717795365822056,0.03544329,0.026960492,0.00022613739,0.17537914,0.003771418,1.0
200000,2.1964047,125.53801169590643,-0.53543156,-0.7631461786334974,-0.7631461786334974,0.013329913,0.021886509,0.0001953219,0.1651073,0.003258853,1.0
250000,2.134423,456.35238095238094,-0.3260269,-0.44690287042231785,-0.44690287042231785,0.006459371,0.02534933,0.00016445022,0.15481675,0.0027453545,1.0
300000,2.1070223,482.42424242424244,-0.10264608,-0.05654143293698629,-0.05654143293698629,0.007644861,0.022003885,0.00013357277,0.14452425,0.0022317597,1.0
350000,2.091923,588.0,-0.036306567,-0.22265059877743668,-0.22265059877743668,0.00358245,0.021473918,0.0001027034,0.13423443,0.0017182985,1.0
400000,2.0958767,541.2359550561798,-0.030653821,-0.11072137097964126,-0.11072137097964126,0.0038994343,0.025032237,7.497773e-05,0.12499256,0.0012571281,1.0
450000,2.0499313,599.1219512195122,-0.025299547,-0.30681709790738615,-0.30681709790738615,0.003973328,0.025643203,4.724949e-05,0.1157498,0.0007959151,1.0
500000,2.031413,563.1684210526316,-0.018112678,-0.11942318175968371,-0.11942318175968371,0.0028463877,0.027040958,1.6427257e-05,0.105475724,0.00028323842,1.0
