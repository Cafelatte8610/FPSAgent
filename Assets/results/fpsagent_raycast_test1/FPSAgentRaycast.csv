Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,2.8869696,388.125,-0.15043555,-0.49375169289608795,-0.49375169289608795,0.0071910247,0.025055053,0.0002845712,0.19485705,0.0047433665,1.0
100000,2.8073957,369.45185185185187,-0.118629836,-0.3199567412818546,-0.3199567412818546,0.010210516,0.025449917,0.0002567685,0.18558949,0.004280916,1.0
150000,2.6806247,408.0650406504065,-0.09678167,-0.5442564756038689,-0.5442564756038689,0.0055268565,0.022012211,0.00022591228,0.17530407,0.0037676734,1.0
200000,2.754445,388.72,-0.106057085,-0.46640642470121385,-0.46640642470121385,0.007871745,0.021133812,0.0001950682,0.16502275,0.003254634,1.0
250000,2.7683287,367.33088235294116,-0.104531206,-0.3826014929834534,-0.3826014929834534,0.008492375,0.024202505,0.00016429074,0.15476355,0.0027427017,1.0
300000,2.7047255,416.260162601626,-0.10580146,-0.5976536865156841,-0.5976536865156841,0.004824365,0.024183774,0.00013348351,0.14449449,0.0022302747,1.0
350000,2.7019901,383.51181102362204,-0.10604602,-0.45290396098546154,-0.45290396098546154,0.0073085492,0.021684911,0.000102596954,0.13419893,0.0017165281,1.0
400000,2.6730464,358.76258992805754,-0.09793514,-0.25261886644622555,-0.25261886644622555,0.010988483,0.022781592,7.479682e-05,0.12493226,0.0012541191,1.0
450000,2.5876641,359.40845070422534,-0.08183903,-0.3369972255680111,-0.3369972255680111,0.008407707,0.024074692,4.7044294e-05,0.11568141,0.00079250184,1.0
500000,2.5201318,374.9923076923077,-0.09091487,-0.400111651813337,-0.400111651813337,0.0067795417,0.023564706,1.6176695e-05,0.10539222,0.00027907078,1.0
